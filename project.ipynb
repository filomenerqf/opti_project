{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad21042",
   "metadata": {},
   "source": [
    "## Optimization for Machine Learning Project\n",
    "\n",
    "This project revisits the course's notebooks by replacing the use of gradients and derivatives with **zeroth-order approximations**. These techniques are randomized and focus on obtaining good approximations in a probabilistic sense, bearing resemblance to finite-difference techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade3d1ac",
   "metadata": {},
   "source": [
    "### An MNIST-based regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1222fc",
   "metadata": {},
   "source": [
    "#### **Question 0**\n",
    "\n",
    "Import the MNIST dataset and build your own subset by making the following changes:\n",
    "\n",
    "1.  Select two digit classes $c_{1}$ and $c_{2}$ such that $\\{c_{1}, c_{2}\\} \\neq \\{0, 1\\}$.\n",
    "2.  Make sure the labels correspond to $0$ for one class and $1$ for the other class.\n",
    "3.  Consider the images as vectors.\n",
    "\n",
    "Given a subset of two classes in MNIST $\\{(\\bm{x_{i}}, y_{i})\\}_{i=1}^{n}$ with $\\bm{x_{i}} \\in \\mathbb{R}^{d}$, $d=28^{2}=784$ and $y_{i} \\in \\{0, +1\\}$, we consider the classification problem:\n",
    "\n",
    "$$\\underset{\\bm{w} \\in \\mathbb{R}^{d}}{\\text{minimize}} \\ f(\\bm{w}) := \\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\bm{w}), \\quad f_{i}(\\bm{w}) := \\left(y_{i} - \\frac{1}{1 + \\exp(-\\bm{x_{i}}^{T}\\bm{w})}\\right)^{2} \\quad (1) \\text{}$$\n",
    "\n",
    "This problem is generally nonconvex. The function $t \\mapsto \\frac{1}{1 + \\exp(-t)}$ is called the sigmoid function, and acts as an approximation to the sign function.\n",
    "\n",
    "For any $i=1,...,n$, the gradient of $f_{i}$ is explicitly given by:\n",
    "$$\\nabla f_{i}(\\bm{w}) = -\\frac{2 \\exp(\\bm{x_{i}}^{T}\\bm{w}) (\\exp(\\bm{x_{i}}^{T}\\bm{w})(y_{i}-1) + y_{i})}{(1 + \\exp(\\bm{x_{i}}^{T}\\bm{w}))^{3}} \\bm{x_{i}} \\quad (2) \\text{}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4287c28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10377a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset\n",
    "\n",
    "to_download = True\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "    to_download = True\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=to_download, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=to_download, transform=transform) \n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "c1, c2 = 2, 3\n",
    "labels_to_keep = [c1, c2]\n",
    "\n",
    "train_mask = torch.tensor(\n",
    "        [(target in labels_to_keep) for target in train_dataset.targets]\n",
    "    )\n",
    "test_mask = torch.tensor(\n",
    "        [(target in labels_to_keep) for target in test_dataset.targets]\n",
    "    )\n",
    "\n",
    "train_dataset.data = train_dataset.data[train_mask]\n",
    "train_dataset.data = train_dataset.data.view(-1, 28*28)\n",
    "train_dataset.targets = train_dataset.targets[train_mask]\n",
    "train_dataset.targets = torch.where(train_dataset.targets == c1, torch.tensor(0), torch.tensor(1))\n",
    "\n",
    "test_dataset.data = test_dataset.data[test_mask]\n",
    "test_dataset.data = test_dataset.data.view(-1, 28*28)\n",
    "test_dataset.targets = test_dataset.targets[test_mask]\n",
    "test_dataset.targets = torch.where(test_dataset.targets == c1, torch.tensor(0), torch.tensor(1))\n",
    "\n",
    "n_train = train_dataset.data.shape[0]\n",
    "n_test = test_dataset.data.shape[0]\n",
    "d = 28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17c5f6",
   "metadata": {},
   "source": [
    "#### **Question 1** \n",
    "\n",
    "Given a data point $(\\bm{x_{i}}, y_{i})$ from your dataset, use the Autograd framework described in the second lab session to implement a code for the function $f_{i}$ that enables computation of $\\nabla f_{i}(\\bm{x})$ for any $\\bm{x}$ through automatic differentiation. Validate your implementation using the explicit formula (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1 / (1 + torch.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26392ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_i(w, x_i, y_i):\n",
    "    return (y_i - sigmoid(torch.dot(w, x_i)))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f_i(w, x_i, y_i):\n",
    "    numerator = -2 * torch.exp(torch.dot(w, x_i)) * (torch.exp(torch.dot(w, x_i)) * (y_i - 1) + y_i)\n",
    "    denominator = (1 + torch.exp(torch.dot(w, x_i)))**3\n",
    "    return numerator / denominator * x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a570bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x_i, y_i = train_dataset.data[i].float(), train_dataset.targets[i].float()\n",
    "w = torch.zeros(d, requires_grad=True)\n",
    "\n",
    "f_i_value = f_i(w, x_i, y_i)\n",
    "\n",
    "f_i_value.backward()\n",
    "grad_f_i_AD = w.grad\n",
    "grad_f_i_explicit = grad_f_i(w.detach(), x_i, y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26047dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Compare the results\n",
    "tolerance = 1e-6\n",
    "comparison_result = torch.allclose(grad_f_i_AD, grad_f_i_explicit, atol=tolerance)\n",
    "print(f\"{comparison_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0302e",
   "metadata": {},
   "source": [
    "**Regularized version** We will also be interested in a regularized version of problem (1), of the form:\n",
    "\n",
    "$$\\underset{\\bm{w} \\in \\mathbb{R}^{d}}{\\text{minimize}} \\ f(\\bm{w}) + \\lambda ||\\bm{w}||_{1} \\quad (3) \\text{}$$\n",
    "where $\\lambda > 0$ and $||\\bm{w}||_{1} = \\sum_{j=1}^{d} |w_{j}|$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74409182",
   "metadata": {},
   "source": [
    "### First-order Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97b09e",
   "metadata": {},
   "source": [
    "#### **Question 2**\n",
    "\n",
    "Adapt the code of gradient descent provided during the lab sessions (or use your own implementation) to run it on problem (1).\n",
    "\n",
    "1.  What convergence rate is expected for gradient descent on this problem? Do you observe this rate empirically?\n",
    "2.  Can you find a good constant value for the stepsize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3153e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w, X, y):\n",
    "    #complete\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87492f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc182692",
   "metadata": {},
   "source": [
    "#### **Question 3**\n",
    "\n",
    "Adapt the code of batch stochastic gradient provided during the lab sessions (or use your own implementation) to compare gradient descent and stochastic gradient on problem (1).\n",
    "\n",
    "1.  Are your results consistent with what the theory predicts?\n",
    "2.  Can you find a good constant stepsize choice for stochastic gradient?\n",
    "3.  What appears to be the best value for the batch size on this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b56ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59614bf0",
   "metadata": {},
   "source": [
    "#### **Question 4**\n",
    "\n",
    "Adapt the code of Adagrad provided during the lab sessions (or use your own implementation) to include that method in the comparison. How does this method compare to the best stochastic gradient method from Question 3 on problem (1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d873f529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba23d1e",
   "metadata": {},
   "source": [
    "#### **Question 5**\n",
    "\n",
    "Consider the best method from Question 4, and apply it to problem (3) using proximal gradient (consider the implementation used as illustration in class, or implement your own). Can you find a value for $\\lambda$ that yields a good yet sparse solution vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba49e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8014c88c",
   "metadata": {},
   "source": [
    "### Zeroth-order Algorithms\n",
    "\n",
    "In this final section, we consider optimization techniques that do not rely on exact gradient calculations (let alone differentiation). Instead, one computes steps by querying only function values.\n",
    "\n",
    "The classical iteration of such a method has the form:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k} \\frac{f(w_{k} + h u_{k}) - f(w_{k})}{h} u_{k} \\quad (4) \\text{}$$\n",
    "where $\\alpha_{k} > 0$ is a stepsize, $h > 0$ plays the role of a finite-difference parameter and $u_{k} \\sim \\mathcal{N}(0, I_{d})$ is a Gaussian random vector. This approach was popularized by Nesterov and algorithms of the form (4) are nowadays referred to as **zeroth-order optimization methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525feb1",
   "metadata": {},
   "source": [
    "#### **Question 6**\n",
    "\n",
    "Implement an algorithm based on iteration (4) and validate your implementation on the linear regression problem from the notebooks.\n",
    "\n",
    "1.  Try different values for $h$ in order to find one that leads to convergence.\n",
    "2.  Try different choices for $\\alpha_{k}$ to obtain the best possible performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c51633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d7a4455",
   "metadata": {},
   "source": [
    "#### **Question 7**\n",
    "\n",
    "It is worth comparing the proposed method (4) with both exact gradient descent (using exact derivatives) and a **finite-difference version** of the method:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k} g_{k} \\quad (79)$$\n",
    "where\n",
    "$$g_{k} = \\left[\\frac{f(w_{k} + h e_{j}) - f(w_{k})}{h}\\right]_{j=1,...,d} \\quad (5) \\text{}$$\n",
    "\n",
    "Compare the performance of your algorithm (4) with that of gradient descent and the finite-difference method (5) on problem (1).\n",
    "\n",
    "1.  How do the variants behave with **constant stepsize**? **Decreasing stepsizes**?\n",
    "2.  Propose a **unit of comparison** (other than the number of iterations) for all three algorithms. Plot the behavior of the methods for a fixed budget of the cost you propose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e2975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a295e5e9",
   "metadata": {},
   "source": [
    "#### **Question 8**\n",
    "\n",
    "In a stochastic setting, there are two ways to implement iteration (4).\n",
    "\n",
    "**Variant 1 (Batch $f_{\\mathcal{S}_{k}}$):** The first considers that a batch of indices $\\mathcal{S}_{k}$ is given at every iteration, and performs:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k} \\frac{f_{\\mathcal{S}_{k}}(w_{k} + h u_{k}) - f_{\\mathcal{S}_{k}}(w_{k})}{h} u_{k} \\quad (6) \\text{}$$\n",
    "where $\\forall \\mathcal{S} \\subset \\{1,...,n\\}$, $f_{\\mathcal{S}}(\\bm{w}) = \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} f_{i}(\\bm{w})$.\n",
    "\n",
    "**Variant 2 (Two Batches $f_{\\mathcal{S}_{k}^{+}}, f_{\\mathcal{S}_{k}}$):** The second approach considers that the same sample cannot be queried twice in a row, yielding the iteration:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k} \\frac{f_{\\mathcal{S}_{k}^{+}}(w_{k} + h u_{k}) - f_{\\mathcal{S}_{k}}(w_{k})}{h} u_{k} \\quad (7) \\text{}$$\n",
    "where $\\mathcal{S}_{k}$ and $\\mathcal{S}_{k}^{+}$ are random batches of same size.\n",
    "\n",
    "Adapt the code from the previous questions to allow for iterations (6) and (7).\n",
    "\n",
    "1.  Compare the variants with **batch size 1** with the deterministic method (4). Do you recover observations from the first-order part of this project?\n",
    "2.  Assuming samples are drawn uniformly at random, find a good value for the **batch size** in both (6) and (7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a9282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fdfd58d",
   "metadata": {},
   "source": [
    "#### **Question 9**\n",
    "\n",
    "1.  Propose an adaptation of your best stochastic zeroth-order variant to the **proximal setting**.\n",
    "2.  Compare the resulting method with the algorithm from Question 5 (Proximal Gradient) on problem (3), with **exact gradients** and **finite-difference gradients**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582a53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9310bd7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Y. Le Cun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. of the IEEE, 86(11):2278-2324, 1998.\n",
    "[2] Yu. Nesterov. Random gradient-free minimization of convex functions. Technical Report 2011/1, CORE, Universit√© Catholique de Louvain, 2011.\n",
    "[3] Yu. Nesterov and V. Spokoiny. Random gradient-free minimization of convex functions. Found. Comput. Math., 17:527-566, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f4ffe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
